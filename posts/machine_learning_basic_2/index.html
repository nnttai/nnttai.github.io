<!doctype html><html dir=ltr lang=en data-theme=white class="html theme--light"><head><title>Machine learning part 2 |
Nguyen Nhi Thanh Tai</title><meta charset=utf-8><meta name=generator content="Hugo 0.118.2"><meta name=viewport content="width=device-width,initial-scale=1,viewport-fit=cover"><meta name=author content="Nguyen Nhi Thanh Tai"><meta name=description content="If you can't explain it simply-You don't understand it well enough."><link rel=stylesheet href=/scss/main.min.51539064fa75e864541ab66487951977ed8d7b7ebfba9a13bd9118a2b3b50381.css integrity="sha256-UVOQZPp16GRUGrZkh5UZd+2Ne36/upoTvZEYorO1A4E=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/css/markupHighlight.min.73ccfdf28df555e11009c13c20ced067af3cb021504cba43644c705930428b00.css integrity="sha256-c8z98o31VeEQCcE8IM7QZ688sCFQTLpDZExwWTBCiwA=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/css/syntax.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css integrity="sha256-47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=" crossorigin=anonymous media=screen><link rel=stylesheet href=/fontawesome/css/fontawesome.min.7d272de35b410fb165377550cdf9c4d3a80fbbcc961e111914e4d5c0eaf5729f.css integrity="sha256-fSct41tBD7FlN3VQzfnE06gPu8yWHhEZFOTVwOr1cp8=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/solid.min.55d8333481b07a08e07cf6f37319753a2b47e99f4c395394c5747b48b495aa9b.css integrity="sha256-VdgzNIGwegjgfPbzcxl1OitH6Z9MOVOUxXR7SLSVqps=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/regular.min.a7448d02590b43449364b6b5922ed9af5410abb4de4238412a830316dedb850b.css integrity="sha256-p0SNAlkLQ0STZLa1ki7Zr1QQq7TeQjhBKoMDFt7bhQs=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/brands.min.9ed75a5d670c953fe4df935937674b4646f92674367e9e66eb995bb04e821647.css integrity="sha256-ntdaXWcMlT/k35NZN2dLRkb5JnQ2fp5m65lbsE6CFkc=" crossorigin=anonymous type=text/css><link rel="shortcut icon" href=/favicon.ico type=image/x-icon><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=canonical href=https://nnttai.github.io/posts/machine_learning_basic_2/><script type=text/javascript src=/js/anatole-header.min.f9132794301a01ff16550ed66763482bd848f62243d278f5e550229a158bfd32.js integrity="sha256-+RMnlDAaAf8WVQ7WZ2NIK9hI9iJD0nj15VAimhWL/TI=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/anatole-theme-switcher.min.d6d329d93844b162e8bed1e915619625ca91687952177552b9b3e211014a2957.js integrity="sha256-1tMp2ThEsWLovtHpFWGWJcqRaHlSF3VSubPiEQFKKVc=" crossorigin=anonymous></script><meta name=twitter:card content="summary"><meta name=twitter:title content="Machine learning part 2"><meta name=twitter:description content><meta property="og:title" content="Machine learning part 2"><meta property="og:description" content><meta property="og:type" content="article"><meta property="og:url" content="https://nnttai.github.io/posts/machine_learning_basic_2/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-09-06T20:20:48+07:00"><meta property="article:modified_time" content="2023-09-06T20:20:48+07:00"><meta property="og:site_name" content="Learn - Do - Share"><meta property="og:see_also" content="https://nnttai.github.io/posts/machine_learning_basic/"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"posts","name":"Machine learning part 2","headline":"Machine learning part 2","alternativeHeadline":"","description":"
      
        


      


    ","inLanguage":"en-us","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/nnttai.github.io\/posts\/machine_learning_basic_2\/"},"author":{"@type":"Person","name":"Nguyen Nhi Thanh Tai"},"creator":{"@type":"Person","name":"Nguyen Nhi Thanh Tai"},"accountablePerson":{"@type":"Person","name":"Nguyen Nhi Thanh Tai"},"copyrightHolder":{"@type":"Person","name":"Nguyen Nhi Thanh Tai"},"copyrightYear":"2023","dateCreated":"2023-09-06T20:20:48.00Z","datePublished":"2023-09-06T20:20:48.00Z","dateModified":"2023-09-06T20:20:48.00Z","publisher":{"@type":"Organization","name":"Nguyen Nhi Thanh Tai","url":"https://nnttai.github.io/","logo":{"@type":"ImageObject","url":"https:\/\/nnttai.github.io\/favicon-32x32.png","width":"32","height":"32"}},"image":[],"url":"https:\/\/nnttai.github.io\/posts\/machine_learning_basic_2\/","wordCount":"952","genre":[],"keywords":["ML","basic"]}</script></head><body class=body><div class=wrapper><aside class=wrapper__sidebar><div class="sidebar
animated fadeInDown"><div class=sidebar__content><div class=sidebar__introduction><img class=sidebar__introduction-profileimage src=/images/profile_pic_edited.jpg alt="profile picture"><div class=sidebar__introduction-title><a href=/>Learn - Do - Share</a></div><div class=sidebar__introduction-description><p>If you can't explain it simply-You don't understand it well enough.</p></div></div><ul class=sidebar__list><li class=sidebar__list-item><a href=https://www.linkedin.com/in/t%C3%A0i-nguy%E1%BB%85n-25556320a/ target=_blank rel="noopener me" aria-label=Linkedin title=Linkedin><i class="fab fa-linkedin fa-2x" aria-hidden=true></i></a></li><li class=sidebar__list-item><a href=mailto:mail@example.com target=_blank rel="noopener me" aria-label=e-mail title=e-mail><i class="fas fa-envelope fa-2x" aria-hidden=true></i></a></li><li class=sidebar__list-item><a href=https://github.com/nnttai target=_blank rel="noopener me" aria-label=github title=github><i class="fab fa-github fa-2x" aria-hidden=true></i></a></li></ul></div><footer class="footer footer__sidebar"><ul class=footer__list><li class=footer__item>&copy;
Nguyen Nhi Thanh Tai
2023</li></ul></footer><script type=text/javascript src=/js/medium-zoom.min.1248fa75275e5ef0cbef27e8c1e27dc507c445ae3a2c7d2ed0be0809555dac64.js integrity="sha256-Ekj6dSdeXvDL7yfoweJ9xQfERa46LH0u0L4ICVVdrGQ=" crossorigin=anonymous></script><script defer type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity=sha384-e/4/LvThKH1gwzXhdbY2AsjR3rm7LHWyhIG5C0jiRfn8AN2eTN5ILeztWw0H9jmN crossorigin=anonymous></script>
<script type=text/x-mathjax-config>
      MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script></div></aside><main class=wrapper__main><header class=header><div class="animated fadeInDown"><a role=button class=navbar-burger data-target=navMenu aria-label=menu aria-expanded=false><span aria-hidden=true class=navbar-burger__line></span>
<span aria-hidden=true class=navbar-burger__line></span>
<span aria-hidden=true class=navbar-burger__line></span></a><nav class=nav><ul class=nav__list id=navMenu><li class=nav__list-item><a href=/ title>Home</a></li><li class=nav__list-item><a href=/posts/ title>Posts</a></li><li class=nav__list-item><a href=/about/ title>About</a></li><li class=nav__list-item><a href=/portfolio/ title>Portfolio</a></li><li class=nav__list-item><div class=optionswitch><input class=optionswitch__picker type=checkbox id=4 hidden>
<label class=optionswitch__label for=4>Accomplishments <i class="fa fa-angle-down" aria-hidden=true></i></label><div class=optionswitch__triangle></div><ul class=optionswitch__list><li class=optionswitch__list-item><a href=/awards/ title>Awards</a></li><li class=optionswitch__list-item><a href=/certifications/ title>Certifications</a></li></ul></div></li><li class=nav__list-item><a href=/contact/ title>Contact</a></li></ul><ul class="nav__list nav__list--end"><li class=nav__list-item><div class=themeswitch><a title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></li></ul></nav></div></header><div class="post
animated fadeInDown"><div class=post__content><h1>Machine Learning Part 2</h1><h3>Table of Contents</h3><nav id=TableOfContents><ul><li><a href=#1-what-is-metric>1. What is metric?</a></li><li><a href=#2-why-metric-important>2. Why metric important?</a><ul><li><a href=#21-train-validation-test-split>2.1 Train validation test split</a></li><li><a href=#22-error-bias-and-variance>2.2 Error, Bias and Variance</a></li><li><a href=#221-error>2.2.1 Error</a></li><li><a href=#222-bias>2.2.2 Bias</a></li><li><a href=#223-variance>2.2.3 Variance</a></li></ul></li><li><a href=#23-why-you-should-need-metric>2.3 Why you should need metric?</a></li></ul></nav><p>In the first blog in this series, we explained about task T and experience E, two parts that help building a model, but to evaluate the model we need something called <em><strong>METRIC</strong></em>.</p><h2 id=1-what-is-metric>1. What is metric?</h2><ul><li>Performance metrics in machine learning are essential for assessing the effectiveness and reliability of the models. They&rsquo;re key element of every machine learning pipeline, allowing developers to fine-tune their algorithms and drive improvement.</li><li>The metrics can be broadly categorized into two main types: regression and classifications.</li></ul><h2 id=2-why-metric-important>2. Why metric important?</h2><ul><li>Some people argue that we can use loss function to evaluate the model performance, through this logic <em>&ldquo;We always want that the loss function as small as possible, so that if the value of loss function small, the model is good&rdquo;</em>. That sound great, however it not easy like that. To make more clearly about why loss function cannot be used to evaluate model performance, we should know some concept in evaluate data:<ul><li>Train validation test split</li><li>Bias</li><li>Variance</li><li>Overfitting</li><li>Underfitting</li></ul></li></ul><h3 id=21-train-validation-test-split>2.1 Train validation test split</h3><ul><li><p>At first, in every machine learning or deep learning project, we need to split the dataset intp three subsets smaller, called:</p><ul><li>Training set: is the data <em>used by the model in training phrase.</em></li><li>Validation set: is the data used to <em>evaluate the training procedure.</em></li><li>Test set: is the data used to <em>evaluate the model performance when the model make predict on unknown input.</em></li></ul></li><li><p>Why we need to split the data?</p><ul><li>To be more imaginable, suppose you are the student with the target is winning the first prize in Olympic Math.</li><li>You go to school and learn knowledge everyday, day by day, so <em>day by day</em> is your learning phrase and <em>knowledge</em> is your train set.</li><li>When the competition day is near, your teacher will give you some sample tests and you do it, so <em>sample tests</em> is the validation test.</li><li>However, the main purpose is do great on real test at the competition, <em>real test</em> is the test set.</li></ul></li><li><p>From the example above, you can see the fact that, the most important thing is get the good result in test set (in modeling phrase only*). To reach the good performance on test set, we should repeat the tuning process many times to have good parameter set. Using metric on those three set will help you do that tuning process in easier and more logical way rather than just tune it randomly.</p></li></ul><p><em>- NOTE:</em> Actually, when we deploy model in production environment, the most inportance is the result when the model is used in real life. Which will be talked in blogs about MLOPs.</p><h3 id=22-error-bias-and-variance>2.2 Error, Bias and Variance</h3><h3 id=221-error>2.2.1 Error</h3><ul><li><p>In machine learning, an error is a measure of how accurately an algorithm can make predictions for particular unknown dataset. There are mainly two type of errors in machine learning, which are:</p><ul><li><strong>Reducible error</strong>: Theses error can be reduced to improve the model accuracy. Such errors can further be classified into bias and variance.</li></ul></li><li><p><strong>Irreducible error</strong>: These errors wills alway be present in the model regardless of which algorithm has been used. The caused of these errors is unknown variables and cannot be reduced.</p></li></ul><figure><img src=/images/bias-and-variance-in-machine-learning2.jpg alt=image><figcaption><p>From Javatpoint&rsquo;s blog</p></figcaption></figure><h3 id=222-bias>2.2.2 Bias</h3><ul><li>Generally, bias is a difference between your model&rsquo;s prediction and expected value. We can have two case of bias:<ul><li>Low bias: The model is low bias when the difference between model&rsquo;s prediction and your expected value is low, which means your model can learn the relationship between data and output.</li><li>High bias: The model is high bias when the difference between model&rsquo;s prediction and your expected value is high, which means the model cannot learn anything from your data. called <strong>Underfit</strong>.</li></ul></li><li><strong>Underfit</strong> is when your model is useless, dont has ability to make good predict.</li><li>High bias mainly occurs due to much simple model, these are some method to reduce high bias:<ul><li>Increase the input features.</li><li>Decrease the regularization term.</li><li>Use more complex model.</li></ul></li></ul><h3 id=223-variance>2.2.3 Variance</h3><ul><li>Variance, generally, estimates how good your model is when your model make prediction on new dataset, for example, you train your model in training set and make prediction on test set. We also have two case of variance:<ul><li>Low variance: when your model doing good on <strong>test set</strong>, so it is low variance</li><li>High variance: when you model doing good on <strong>train set</strong>, but bad on <strong>test set</strong>, so it call high variance. High variance usually because your model is so complicated, and it can learn by heart the relationship in each data point in training set, that why it good on training set. Also called <strong>Overfit</strong>.</li></ul></li><li><strong>Overfit</strong> is when your model doing so good on train set but so bad in test set.</li><li>Ways to reduce high variance:<ul><li>Reduce the input feature or number of parameter in model.</li><li>Use simpler model or algorithm.</li><li>Increase the training data.</li><li>Increase the regularization term.</li></ul></li></ul><h2 id=23-why-you-should-need-metric>2.3 Why you should need metric?</h2><ul><li>The goal of building a model is creating a model that has ability to make good prediction on data which model haven&rsquo;t seen before. Because building a good model is a iterative process, you evaluate the model on test set and tuning it until it reach good performance. The reason why you cannot use loss function are:<ul><li>Loss function cannot be used for checking that the model is underfit or overfit.</li><li>Loss function can just be used for checking the model is optimized or not and metric is used to know that the model is doing good or not.</li><li>You choose the model and use loss function to optimize the model after that you use the metric the evaluate that the optimization process is good or not by use that model to make prediction on test set.</li></ul></li></ul><h3>Posts in this series</h3><ul><li><a href=/posts/machine_learning_basic_2/>Machine Learning Part 2</a></li><li><a href=/posts/machine_learning_basic/>Machine Learning Part 1</a></li></ul></div><div class=post__footer><span><a class=tag href=/tags/ml/>ML</a><a class=tag href=/tags/basic/>basic</a></span></div></div></main></div><footer class="footer footer__base"><ul class=footer__list><li class=footer__item>&copy;
Nguyen Nhi Thanh Tai
2023</li></ul></footer><script type=text/javascript src=/js/medium-zoom.min.1248fa75275e5ef0cbef27e8c1e27dc507c445ae3a2c7d2ed0be0809555dac64.js integrity="sha256-Ekj6dSdeXvDL7yfoweJ9xQfERa46LH0u0L4ICVVdrGQ=" crossorigin=anonymous></script><script defer type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity=sha384-e/4/LvThKH1gwzXhdbY2AsjR3rm7LHWyhIG5C0jiRfn8AN2eTN5ILeztWw0H9jmN crossorigin=anonymous></script>
<script type=text/x-mathjax-config>
      MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script></body></html>